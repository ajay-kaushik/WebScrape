{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Top Repositoried for Topics on GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What is Web Scraping ?\n",
    "\n",
    "- Web Scripting is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is GitHub ?\n",
    "\n",
    "- GitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere.\n",
    "\n",
    "### Problem Statement \n",
    "\n",
    "- We are gonna scrap top 30 topic of a Github and for each topic will get top 25 repositories from the topic page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the steps we'll follow\n",
    "\n",
    "- We are going to scrape https://github.com/topics.\n",
    "- We'll get a list of topics. For each topic, we'll get topic titles, topic page URL and topic description.\n",
    "- For each topic, we'll get the top 25 repositories in the from the topic page.\n",
    "- For each repository, we'll grab the repo name, username, stars and report URL\n",
    "- For each topic we'll create a CSV file in the following format:\n",
    "\n",
    "  Repo Name,Username,Stars,Repo URL\n",
    "  \n",
    "  three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
    "  \n",
    "  libgdx,libgdx,18300,https://github.com/libgdx/libgdx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Srape the list of topics from Github\n",
    " \n",
    " - Use requests to download the page\n",
    " - Use BS4 to parse and extract information\n",
    " - Convert to Pandas dataframe\n",
    "\n",
    "Let's write a function to download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_topics_page():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    # download a page\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topics_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "def get_topic_page(topic_url):\n",
    "    # Download the page\n",
    "    response = requests.get(topic_url)\n",
    "    if response.status_code!= 200:\n",
    "        raise Exception('Failed to load{}',format(topic_url))\n",
    "    \n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc\n",
    "\n",
    "\n",
    "def get_repo_info(h1_tags, star_tag):\n",
    "    a_tags = h1_tags.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    base_url = 'https://github.com'\n",
    "    repo_url= base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name,stars, repo_url\n",
    "\n",
    "def get_topic_repos(topic_doc):\n",
    "    h1_selection_class = 'f3 color-text-secondary text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h1',{'class',h1_selection_class})\n",
    "    star_tags = topic_doc.find_all('a',{'class':'social-count float-none'})\n",
    "   \n",
    "    topic_repos_dict = {\n",
    "    'username': [],\n",
    "    'repo_name': [],\n",
    "    'stars': [],\n",
    "    'repo_url': []\n",
    "    }\n",
    "\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "    \n",
    "    return pd.DataFrame(topic_repos_dict)\n",
    "\n",
    "\n",
    "def scrape_topic(topic_url, path):\n",
    "    if os.path.exists(path):\n",
    "        print(\"The file {} already exists. Skipping...\".format(path))\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p',{'class':selection_class})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text.strip())\n",
    "\n",
    "    return topic_titles\n",
    "\n",
    "def get_topic_descs(doc):\n",
    "    selection_class = 'f5 color-text-secondary mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p',{'class':selection_class})\n",
    "    topic_descs = []\n",
    "\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "\n",
    "    return topic_descs\n",
    "\n",
    "def get_topic_urls(doc):\n",
    "    topic_link_tags = doc.find_all('a', {'class': 'd-flex no-underline'})\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "\n",
    "    return topic_urls\n",
    "\n",
    "\n",
    "def scrape_topics():\n",
    "    topic_url = 'https://github.com/topics'\n",
    "    response = requests.get(topic_url)\n",
    "\n",
    "    if response.status_code!= 200:\n",
    "        raise Exception('Failed to load{}',format(topic_url))\n",
    "    \n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "   \n",
    "    topics_dict = {\n",
    "        'title': get_topic_titles(topic_doc),\n",
    "        'description': get_topic_descs(topic_doc),\n",
    "        'url': get_topic_urls(topic_doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print('Scraping list of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scraping top repositories for \"{}\"'.format(row['title']))\n",
    "        scrape_topic(row['url'], 'data/{}.csv'.format(row['title']))\n",
    "    print('Scraping top repositories are completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "Scraping top repositories for \"3D\"\n",
      "The file data/3D.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Ajax\"\n",
      "The file data/Ajax.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Algorithm\"\n",
      "The file data/Algorithm.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Amp\"\n",
      "The file data/Amp.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Android\"\n",
      "The file data/Android.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Angular\"\n",
      "The file data/Angular.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Ansible\"\n",
      "The file data/Ansible.csv already exists. Skipping...\n",
      "Scraping top repositories for \"API\"\n",
      "The file data/API.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Arduino\"\n",
      "The file data/Arduino.csv already exists. Skipping...\n",
      "Scraping top repositories for \"ASP.NET\"\n",
      "The file data/ASP.NET.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Atom\"\n",
      "The file data/Atom.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Awesome Lists\"\n",
      "The file data/Awesome Lists.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Amazon Web Services\"\n",
      "The file data/Amazon Web Services.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Azure\"\n",
      "The file data/Azure.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Babel\"\n",
      "The file data/Babel.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Bash\"\n",
      "The file data/Bash.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Bitcoin\"\n",
      "The file data/Bitcoin.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Bootstrap\"\n",
      "The file data/Bootstrap.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Bot\"\n",
      "The file data/Bot.csv already exists. Skipping...\n",
      "Scraping top repositories for \"C\"\n",
      "The file data/C.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Chrome\"\n",
      "The file data/Chrome.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Chrome extension\"\n",
      "The file data/Chrome extension.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Command line interface\"\n",
      "The file data/Command line interface.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Clojure\"\n",
      "The file data/Clojure.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Code quality\"\n",
      "The file data/Code quality.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Code review\"\n",
      "The file data/Code review.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Compiler\"\n",
      "The file data/Compiler.csv already exists. Skipping...\n",
      "Scraping top repositories for \"Continuous integration\"\n",
      "The file data/Continuous integration.csv already exists. Skipping...\n",
      "Scraping top repositories for \"COVID-19\"\n",
      "The file data/COVID-19.csv already exists. Skipping...\n",
      "Scraping top repositories for \"C++\"\n",
      "The file data/C++.csv already exists. Skipping...\n",
      "Scraping top repositories are completed\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
